{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d584c-641c-466e-8d32-f5d94b216ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable pyLDAvis for notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\\\Users\\\\sahel\\\\Downloads\\\\archive (5)\\\\rows.csv\"\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"<class 'pandas.core.frame.DataFrame'>\")\n",
    "print(f\"RangeIndex: {len(df)} entries, 0 to {len(df)-1}\")\n",
    "print(f\"Data columns (total {len(df.columns)} columns):\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    non_null = df[col].notna().sum()\n",
    "    dtype = df[col].dtype\n",
    "    print(f\" {i:<2} {col:<35} {non_null} non-null  {dtype}\")\n",
    "print(f\"dtypes: {df.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "print(df.head())\n",
    "\n",
    "# Check for complaint narrative column\n",
    "complaint_col = 'Consumer complaint narrative'\n",
    "if complaint_col not in df.columns:\n",
    "    narrative_cols = [col for col in df.columns if 'narrative' in col.lower() or 'complaint' in col.lower()]\n",
    "    if narrative_cols:\n",
    "        complaint_col = narrative_cols[0]\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nMissing values in {complaint_col}: {df[complaint_col].isna().sum()}\")\n",
    "df_clean = df.dropna(subset=[complaint_col]).copy()\n",
    "print(f\"Dataset size after removing missing narratives: {len(df_clean)}\")\n",
    "\n",
    "# Sample data for faster processing\n",
    "sample_size = min(5000, len(df_clean))\n",
    "df_sample = df_clean.sample(sample_size, random_state=42).copy()\n",
    "print(f\"\\nWorking with {sample_size} samples\")\n",
    "\n",
    "# Load spaCy model\n",
    "print(\"\\nLoading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and len(token.text) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess texts\n",
    "print(\"\\nPreprocessing texts...\")\n",
    "df_sample['cleaned_text'] = df_sample[complaint_col].apply(preprocess_text)\n",
    "df_sample = df_sample[df_sample['cleaned_text'].str.len() > 10].copy()\n",
    "print(f\"Samples after cleaning: {len(df_sample)}\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"\\nApplying TF-IDF vectorization...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_sample['cleaned_text'])\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Count Vectorization \n",
    "print(\"\\nApplying Count vectorization...\")\n",
    "count_vectorizer = CountVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
    "count_matrix = count_vectorizer.fit_transform(df_sample['cleaned_text'])\n",
    "print(f\"Count matrix shape: {count_matrix.shape}\")\n",
    "\n",
    "# spaCy Embeddings\n",
    "print(\"\\nGenerating spaCy embeddings...\")\n",
    "spacy_available = False\n",
    "try:\n",
    "    nlp_md = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    def get_spacy_embedding(text):\n",
    "        doc = nlp_md(text)\n",
    "        return doc.vector\n",
    "    \n",
    "    df_sample['spacy_embedding'] = df_sample['cleaned_text'].apply(get_spacy_embedding)\n",
    "    spacy_embeddings = np.vstack(df_sample['spacy_embedding'].values)\n",
    "    print(f\"spaCy embeddings shape: {spacy_embeddings.shape}\")\n",
    "    spacy_available = True\n",
    "except:\n",
    "    print(\"spaCy medium model not available. Skipping spaCy embeddings.\")\n",
    "\n",
    "# Dimensionality Reduction with PCA\n",
    "print(\"\\nApplying PCA for visualization...\")\n",
    "pca_tfidf = PCA(n_components=2)\n",
    "tfidf_2d = pca_tfidf.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Visualization - TF-IDF\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "scatter = ax.scatter(tfidf_2d[:, 0], tfidf_2d[:, 1], alpha=0.6, s=30, c=range(len(tfidf_2d)), cmap='viridis')\n",
    "ax.set_title(\"TF-IDF Embeddings (PCA)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Principal Component 1\", fontsize=11)\n",
    "ax.set_ylabel(\"Principal Component 2\", fontsize=11)\n",
    "plt.colorbar(scatter, ax=ax, label='Sample Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if spacy_available:\n",
    "    pca_spacy = PCA(n_components=2)\n",
    "    spacy_2d = pca_spacy.fit_transform(spacy_embeddings)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    scatter = ax.scatter(spacy_2d[:, 0], spacy_2d[:, 1], alpha=0.6, s=30, c=range(len(spacy_2d)), cmap='plasma')\n",
    "    ax.set_title(\"spaCy Embeddings (PCA)\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Principal Component 1\", fontsize=11)\n",
    "    ax.set_ylabel(\"Principal Component 2\", fontsize=11)\n",
    "    plt.colorbar(scatter, ax=ax, label='Sample Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cosine Similarity Analysis\n",
    "print(\"\\nComputing cosine similarity...\")\n",
    "sample_subset = min(100, len(df_sample))\n",
    "tfidf_cosine_sim = cosine_similarity(tfidf_matrix[:sample_subset])\n",
    "np.fill_diagonal(tfidf_cosine_sim, 0)\n",
    "most_similar_idx = np.unravel_index(np.argmax(tfidf_cosine_sim), tfidf_cosine_sim.shape)\n",
    "\n",
    "print(f\"\\nMost similar complaints (indices: {most_similar_idx}):\")\n",
    "print(f\"\\nComplaint 1:\\n{df_sample.iloc[most_similar_idx[0]][complaint_col][:300]}...\")\n",
    "print(f\"\\nComplaint 2:\\n{df_sample.iloc[most_similar_idx[1]][complaint_col][:300]}...\")\n",
    "print(f\"\\nSimilarity score: {tfidf_cosine_sim[most_similar_idx]:.4f}\")\n",
    "\n",
    "# K-Means Clustering\n",
    "print(\"\\nPerforming K-Means clustering...\")\n",
    "num_clusters = 5\n",
    "embeddings_for_clustering = spacy_embeddings if spacy_available else tfidf_matrix.toarray()\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "df_sample['cluster'] = kmeans.fit_predict(embeddings_for_clustering)\n",
    "\n",
    "# Distribution of clusters\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(df_sample['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Extract top words per cluster\n",
    "print(f\"\\nTop words per cluster:\")\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "cluster_tfidf = np.zeros((num_clusters, tfidf_matrix.shape[1]))\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_mask = (df_sample['cluster'] == i).values\n",
    "    cluster_tfidf[i] = tfidf_matrix[cluster_mask].mean(axis=0).A1\n",
    "\n",
    "top_n = 10\n",
    "cluster_keywords = {}\n",
    "for i in range(num_clusters):\n",
    "    top_words_idx = cluster_tfidf[i].argsort()[-top_n:][::-1]\n",
    "    top_words = [feature_names[j] for j in top_words_idx]\n",
    "    cluster_keywords[i] = top_words\n",
    "    print(f\"\\nCluster {i} ({df_sample[df_sample['cluster']==i].shape[0]} complaints):\")\n",
    "    print(f\"  {', '.join(top_words)}\")\n",
    "\n",
    "# Visualize clusters with PCA\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "embedding_type = \"spaCy\" if spacy_available else \"TF-IDF\"\n",
    "embeddings_2d = spacy_2d if spacy_available else tfidf_2d\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_points = embeddings_2d[df_sample['cluster'] == i]\n",
    "    ax.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "              label=f'Cluster {i}', alpha=0.6, s=40)\n",
    "\n",
    "ax.set_title(f\"K-Means Clustering ({embedding_type} + PCA)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Principal Component 1\", fontsize=11)\n",
    "ax.set_ylabel(\"Principal Component 2\", fontsize=11)\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word clouds for clusters\n",
    "print(\"\\nGenerating word clouds for clusters...\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_texts = ' '.join(df_sample[df_sample['cluster'] == i]['cleaned_text'].values)\n",
    "    wordcloud = WordCloud(width=400, height=300, background_color='white', \n",
    "                         colormap='viridis', max_words=50).generate(cluster_texts)\n",
    "    \n",
    "    axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[i].set_title(f'Cluster {i}', fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "axes[5].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Preliminary Topic Modeling\n",
    "print(\"\\nPerforming preliminary Latent Dirichlet Allocation (LDA)...\")\n",
    "n_topics_lda = 5\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics_lda, random_state=42, max_iter=20)\n",
    "lda_topics = lda_model.fit_transform(count_matrix)\n",
    "\n",
    "def display_topics(model, feature_names, num_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-num_words-1:-1]]\n",
    "        topics.append(top_words)\n",
    "        print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "    return topics\n",
    "\n",
    "print(\"\\nPreliminary LDA Topics:\")\n",
    "lda_topic_words = display_topics(lda_model, count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nGenerating LDA topic distribution heatmap...\")\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sample_docs = min(50, len(lda_topics))\n",
    "sns.heatmap(lda_topics[:sample_docs].T, cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Topic Probability'})\n",
    "ax.set_xlabel(\"Document Index\", fontsize=11)\n",
    "ax.set_ylabel(\"Topic\", fontsize=11)\n",
    "ax.set_title(\"LDA Topic Distribution (First 50 Documents)\", fontsize=14, fontweight='bold')\n",
    "ax.set_yticklabels([f'Topic {i+1}' for i in range(n_topics_lda)], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerforming preliminary Non-negative Matrix Factorization (NMF)...\")\n",
    "n_topics_nmf = 5\n",
    "nmf_model = NMF(n_components=n_topics_nmf, random_state=42, max_iter=200)\n",
    "nmf_topics = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(\"\\nPreliminary NMF Topics:\")\n",
    "nmf_topic_words = display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nGenerating NMF topic distribution heatmap...\")\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sample_docs = min(50, len(nmf_topics))\n",
    "sns.heatmap(nmf_topics[:sample_docs].T, cmap='BuPu', ax=ax, cbar_kws={'label': 'Topic Weight'})\n",
    "ax.set_xlabel(\"Document Index\", fontsize=11)\n",
    "ax.set_ylabel(\"Topic\", fontsize=11)\n",
    "ax.set_title(\"NMF Topic Distribution (First 50 Documents)\", fontsize=14, fontweight='bold')\n",
    "ax.set_yticklabels([f'Topic {i+1}' for i in range(n_topics_nmf)], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Coherence Score Analysis for Optimal K\n",
    "print(\"\\nFinding optimal number of topics using coherence approaches...\")\n",
    "topic_range = range(2, 11)\n",
    "perplexity_scores = []\n",
    "coherence_cv_scores = []\n",
    "coherence_umass_scores = []\n",
    "\n",
    "texts_for_coherence = df_sample['cleaned_text'].str.split().tolist()\n",
    "dictionary_coherence = corpora.Dictionary(texts_for_coherence)\n",
    "corpus_coherence = [dictionary_coherence.doc2bow(text) for text in texts_for_coherence]\n",
    "\n",
    "print(\"\\nCalculating Perplexity, C_v Coherence, and UMass Coherence...\")\n",
    "for n_topics in topic_range:\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=20)\n",
    "    lda.fit(count_matrix)\n",
    "    perplexity_scores.append(lda.perplexity(count_matrix))\n",
    "    \n",
    "    feature_names = count_vectorizer.get_feature_names_out()\n",
    "    lda_topics_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_indices = topic.argsort()[-10:][::-1]\n",
    "        topic_words = [feature_names[i] for i in top_indices]\n",
    "        lda_topics_words.append(topic_words)\n",
    "    \n",
    "    cm_cv = CoherenceModel(\n",
    "        topics=lda_topics_words,\n",
    "        texts=texts_for_coherence,\n",
    "        dictionary=dictionary_coherence,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherence_cv_scores.append(cm_cv.get_coherence())\n",
    "    \n",
    "    cm_umass = CoherenceModel(\n",
    "        topics=lda_topics_words,\n",
    "        texts=texts_for_coherence,\n",
    "        dictionary=dictionary_coherence,\n",
    "        coherence='u_mass'\n",
    "    )\n",
    "    coherence_umass_scores.append(cm_umass.get_coherence())\n",
    "    \n",
    "    print(f\"Topics: {n_topics}, Perplexity: {perplexity_scores[-1]:.2f}, C_v: {coherence_cv_scores[-1]:.4f}, UMass: {coherence_umass_scores[-1]:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes[0,0].plot(topic_range, perplexity_scores, marker='o', linewidth=2, markersize=8, color='blue')\n",
    "axes[0,0].set_xlabel(\"Number of Topics (k)\")\n",
    "axes[0,0].set_ylabel(\"Perplexity Score\")\n",
    "axes[0,0].set_title(\"LDA Perplexity vs. k\", fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].plot(topic_range, coherence_cv_scores, marker='o', linewidth=2, markersize=8, color='green')\n",
    "axes[0,1].set_xlabel(\"Number of Topics (k)\")\n",
    "axes[0,1].set_ylabel(\"C_v Coherence Score\")\n",
    "axes[0,1].set_title(\"LDA C_v Coherence vs. k\", fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,0].plot(topic_range, coherence_umass_scores, marker='o', linewidth=2, markersize=8, color='red')\n",
    "axes[1,0].set_xlabel(\"Number of Topics (k)\")\n",
    "axes[1,0].set_ylabel(\"UMass Coherence Score\")\n",
    "axes[1,0].set_title(\"LDA UMass Coherence vs. k\", fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,1].plot(topic_range, coherence_cv_scores, marker='o', label='C_v', linewidth=2)\n",
    "axes[1,1].plot(topic_range, coherence_umass_scores, marker='s', label='UMass', linewidth=2)\n",
    "axes[1,1].set_xlabel(\"Number of Topics (k)\")\n",
    "axes[1,1].set_ylabel(\"Coherence Score\")\n",
    "axes[1,1].set_title(\"Combined Coherence Metrics\", fontweight='bold')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "optimal_topics_cv = topic_range[np.argmax(coherence_cv_scores)]\n",
    "optimal_topics_umass = topic_range[np.argmax(coherence_umass_scores)]\n",
    "optimal_topics = optimal_topics_cv \n",
    "\n",
    "# Final Phase Reporting and Model Training\n",
    "print(\"\\nFINAL PHASE REPORT: IDENTIFIED TOPICS OVERVIEW\")\n",
    "print(f\"Methodology: Dual Coherence Score Evaluation (C_v + UMass)\")\n",
    "print(f\"Optimal number of topics (k) identified by C_v Coherence: {optimal_topics}\")\n",
    "print(f\"Max C_v Coherence Score achieved: {max(coherence_cv_scores):.4f}\")\n",
    "print(f\"UMass optimal k: {optimal_topics_umass} (score: {max(coherence_umass_scores):.4f})\")\n",
    "print(f\"Recommendation: Proceeding with k={optimal_topics} for final interpretation.\")\n",
    "\n",
    "print(\"\\nTraining FINAL Gensim LDA model with optimal topics...\")\n",
    "texts = df_sample['cleaned_text'].str.split().tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_gensim_final = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=optimal_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "print(\"\\nFINAL IDENTIFIED TOPICS OVERVIEW:\")\n",
    "for idx, topic in lda_gensim_final.print_topics(num_words=15):\n",
    "    print(f\"FINAL TOPIC {idx+1}: {topic}\")\n",
    "\n",
    "print(\"\\nPreparing final interactive visualization...\")\n",
    "lda_vis_final = gensimvis.prepare(lda_gensim_final, corpus, dictionary, sort_topics=False)\n",
    "html_file = 'final_lda_visualization.html'\n",
    "pyLDAvis.save_html(lda_vis_final, html_file)\n",
    "print(f\"Visualization saved to {html_file}\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(lda_vis_final)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nFINAL NMF comparison with optimal topics...\")\n",
    "nmf_final = NMF(n_components=optimal_topics, random_state=42, max_iter=200)\n",
    "nmf_topics_final = nmf_final.fit_transform(tfidf_matrix)\n",
    "display_topics(nmf_final, tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Comprehensive Summary of All Approaches Used\n",
    "print(\"\\nCOMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(\"The following analytical approaches were successfully implemented:\")\n",
    "print(\"1. Data Preprocessing: Cleaned text using spaCy (lemmatization, stop-word removal, and alpha filtering).\")\n",
    "print(\"2. Text Vectorization: Implemented both TF-IDF and Count Vectorization.\")\n",
    "print(\"3. Dense Embeddings: Generated word vectors using spaCy's pre-trained medium language model.\")\n",
    "print(\"4. Dimensionality Reduction: Applied PCA (Principal Component Analysis) to visualize clusters in 2D.\")\n",
    "print(\"5. Similarity Analysis: Conducted Cosine Similarity to identify related document narratives.\")\n",
    "print(\"6. Unsupervised Clustering: Implemented K-Means clustering and generated Word Clouds for cluster interpretation.\")\n",
    "print(\"7. Topic Modeling (Preliminary): Applied LDA (scikit-learn) and NMF for initial topic discovery.\")\n",
    "print(\"8. Model Validation (Coherence): Performed a multi-metric sweep (Perplexity, C_v Coherence, and UMass Coherence).\")\n",
    "print(\"9. Optimization: Mathematically identified the optimal topic count (k) using the C_v metric.\")\n",
    "print(\"10. Final Modeling & Visualization: Trained an optimized Gensim LDA model and generated an interactive pyLDAvis report.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
