{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75805ea-5b85-4f90-9dc0-cc1c486e7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable pyLDAvis for notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\sahel\\Downloads\\archive\\rows.csv\"\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"<class 'pandas.core.frame.DataFrame'>\")\n",
    "print(f\"RangeIndex: {len(df)} entries, 0 to {len(df)-1}\")\n",
    "print(f\"Data columns (total {len(df.columns)} columns):\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    non_null = df[col].notna().sum()\n",
    "    dtype = df[col].dtype\n",
    "    print(f\" {i:<2} {col:<35} {non_null} non-null  {dtype}\")\n",
    "print(f\"dtypes: {df.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "print(df.head())\n",
    "\n",
    "# Check for complaint narrative column\n",
    "complaint_col = 'Consumer complaint narrative'\n",
    "if complaint_col not in df.columns:\n",
    "    narrative_cols = [col for col in df.columns if 'narrative' in col.lower() or 'complaint' in col.lower()]\n",
    "    if narrative_cols:\n",
    "        complaint_col = narrative_cols[0]\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nMissing values in {complaint_col}: {df[complaint_col].isna().sum()}\")\n",
    "df_clean = df.dropna(subset=[complaint_col]).copy()\n",
    "print(f\"Dataset size after removing missing narratives: {len(df_clean)}\")\n",
    "\n",
    "# Sample data for faster processing\n",
    "sample_size = min(5000, len(df_clean))\n",
    "df_sample = df_clean.sample(sample_size, random_state=42).copy()\n",
    "print(f\"\\nWorking with {sample_size} samples\")\n",
    "\n",
    "# Load spaCy model\n",
    "print(\"\\nLoading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and len(token.text) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess texts\n",
    "print(\"\\nPreprocessing texts...\")\n",
    "df_sample['cleaned_text'] = df_sample[complaint_col].apply(preprocess_text)\n",
    "df_sample = df_sample[df_sample['cleaned_text'].str.len() > 10].copy()\n",
    "print(f\"Samples after cleaning: {len(df_sample)}\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"\\nApplying TF-IDF vectorization...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_sample['cleaned_text'])\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Count Vectorization for LDA\n",
    "print(\"\\nApplying Count vectorization...\")\n",
    "count_vectorizer = CountVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
    "count_matrix = count_vectorizer.fit_transform(df_sample['cleaned_text'])\n",
    "print(f\"Count matrix shape: {count_matrix.shape}\")\n",
    "\n",
    "# spaCy Embeddings\n",
    "print(\"\\nGenerating spaCy embeddings...\")\n",
    "try:\n",
    "    nlp_md = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    def get_spacy_embedding(text):\n",
    "        doc = nlp_md(text)\n",
    "        return doc.vector\n",
    "    \n",
    "    df_sample['spacy_embedding'] = df_sample['cleaned_text'].apply(get_spacy_embedding)\n",
    "    spacy_embeddings = np.vstack(df_sample['spacy_embedding'].values)\n",
    "    print(f\"spaCy embeddings shape: {spacy_embeddings.shape}\")\n",
    "    spacy_available = True\n",
    "except:\n",
    "    print(\"spaCy medium model not available. Skipping spaCy embeddings.\")\n",
    "    spacy_available = False\n",
    "\n",
    "# Dimensionality Reduction with PCA\n",
    "print(\"\\nApplying PCA for visualization...\")\n",
    "pca_tfidf = PCA(n_components=2)\n",
    "tfidf_2d = pca_tfidf.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Visualization - TF-IDF\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "scatter = ax.scatter(tfidf_2d[:, 0], tfidf_2d[:, 1], alpha=0.6, s=30, c=range(len(tfidf_2d)), cmap='viridis')\n",
    "ax.set_title(\"TF-IDF Embeddings (PCA)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Principal Component 1\", fontsize=11)\n",
    "ax.set_ylabel(\"Principal Component 2\", fontsize=11)\n",
    "plt.colorbar(scatter, ax=ax, label='Sample Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if spacy_available:\n",
    "    pca_spacy = PCA(n_components=2)\n",
    "    spacy_2d = pca_spacy.fit_transform(spacy_embeddings)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    scatter = ax.scatter(spacy_2d[:, 0], spacy_2d[:, 1], alpha=0.6, s=30, c=range(len(spacy_2d)), cmap='plasma')\n",
    "    ax.set_title(\"spaCy Embeddings (PCA)\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Principal Component 1\", fontsize=11)\n",
    "    ax.set_ylabel(\"Principal Component 2\", fontsize=11)\n",
    "    plt.colorbar(scatter, ax=ax, label='Sample Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cosine Similarity Analysis\n",
    "print(\"\\nComputing cosine similarity...\")\n",
    "sample_subset = min(100, len(df_sample))\n",
    "tfidf_cosine_sim = cosine_similarity(tfidf_matrix[:sample_subset])\n",
    "np.fill_diagonal(tfidf_cosine_sim, 0)\n",
    "most_similar_idx = np.unravel_index(np.argmax(tfidf_cosine_sim), tfidf_cosine_sim.shape)\n",
    "\n",
    "print(f\"\\nMost similar complaints (indices: {most_similar_idx}):\")\n",
    "print(f\"\\nComplaint 1:\\n{df_sample.iloc[most_similar_idx[0]][complaint_col][:300]}...\")\n",
    "print(f\"\\nComplaint 2:\\n{df_sample.iloc[most_similar_idx[1]][complaint_col][:300]}...\")\n",
    "print(f\"\\nSimilarity score: {tfidf_cosine_sim[most_similar_idx]:.4f}\")\n",
    "\n",
    "# K-Means Clustering\n",
    "print(\"\\nPerforming K-Means clustering...\")\n",
    "num_clusters = 5\n",
    "embeddings_for_clustering = spacy_embeddings if spacy_available else tfidf_matrix.toarray()\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "df_sample['cluster'] = kmeans.fit_predict(embeddings_for_clustering)\n",
    "\n",
    "# Distribution of clusters\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(df_sample['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Extract top words per cluster\n",
    "print(f\"\\nTop words per cluster:\")\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "cluster_tfidf = np.zeros((num_clusters, tfidf_matrix.shape[1]))\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_mask = (df_sample['cluster'] == i).values\n",
    "    cluster_tfidf[i] = tfidf_matrix[cluster_mask].mean(axis=0).A1\n",
    "\n",
    "top_n = 10\n",
    "cluster_keywords = {}\n",
    "for i in range(num_clusters):\n",
    "    top_words_idx = cluster_tfidf[i].argsort()[-top_n:][::-1]\n",
    "    top_words = [feature_names[j] for j in top_words_idx]\n",
    "    cluster_keywords[i] = top_words\n",
    "    print(f\"\\nCluster {i} ({df_sample[df_sample['cluster']==i].shape[0]} complaints):\")\n",
    "    print(f\"  {', '.join(top_words)}\")\n",
    "\n",
    "# Visualize clusters with PCA\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "embedding_type = \"spaCy\" if spacy_available else \"TF-IDF\"\n",
    "embeddings_2d = spacy_2d if spacy_available else tfidf_2d\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_points = embeddings_2d[df_sample['cluster'] == i]\n",
    "    ax.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "              label=f'Cluster {i}', alpha=0.6, s=40)\n",
    "\n",
    "ax.set_title(f\"K-Means Clustering ({embedding_type} + PCA)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Principal Component 1\", fontsize=11)\n",
    "ax.set_ylabel(\"Principal Component 2\", fontsize=11)\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word clouds for clusters\n",
    "print(\"\\nGenerating word clouds for clusters...\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_texts = ' '.join(df_sample[df_sample['cluster'] == i]['cleaned_text'].values)\n",
    "    wordcloud = WordCloud(width=400, height=300, background_color='white', \n",
    "                         colormap='viridis', max_words=50).generate(cluster_texts)\n",
    "    \n",
    "    axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[i].set_title(f'Cluster {i}', fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "axes[5].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Topic Modeling - LDA\n",
    "print(\"\\n\\nPerforming Latent Dirichlet Allocation (LDA)...\")\n",
    "n_topics_lda = 5\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics_lda, random_state=42, max_iter=20)\n",
    "lda_topics = lda_model.fit_transform(count_matrix)\n",
    "\n",
    "def display_topics(model, feature_names, num_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-num_words-1:-1]]\n",
    "        topics.append(top_words)\n",
    "        print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "    return topics\n",
    "\n",
    "print(\"\\nLDA Topics:\")\n",
    "lda_topic_words = display_topics(lda_model, count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Topic distribution heatmap for LDA\n",
    "print(\"\\nGenerating LDA topic distribution heatmap...\")\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sample_docs = min(50, len(lda_topics))\n",
    "sns.heatmap(lda_topics[:sample_docs].T, cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Topic Probability'})\n",
    "ax.set_xlabel(\"Document Index\", fontsize=11)\n",
    "ax.set_ylabel(\"Topic\", fontsize=11)\n",
    "ax.set_title(\"LDA Topic Distribution (First 50 Documents)\", fontsize=14, fontweight='bold')\n",
    "ax.set_yticklabels([f'Topic {i+1}' for i in range(n_topics_lda)], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Topic Modeling - NMF\n",
    "print(\"\\n\\nPerforming Non-negative Matrix Factorization (NMF)...\")\n",
    "n_topics_nmf = 5\n",
    "nmf_model = NMF(n_components=n_topics_nmf, random_state=42, max_iter=200)\n",
    "nmf_topics = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(\"\\nNMF Topics:\")\n",
    "nmf_topic_words = display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Topic distribution heatmap for NMF\n",
    "print(\"\\nGenerating NMF topic distribution heatmap...\")\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sample_docs = min(50, len(nmf_topics))\n",
    "sns.heatmap(nmf_topics[:sample_docs].T, cmap='BuPu', ax=ax, cbar_kws={'label': 'Topic Weight'})\n",
    "ax.set_xlabel(\"Document Index\", fontsize=11)\n",
    "ax.set_ylabel(\"Topic\", fontsize=11)\n",
    "ax.set_title(\"NMF Topic Distribution (First 50 Documents)\", fontsize=14, fontweight='bold')\n",
    "ax.set_yticklabels([f'Topic {i+1}' for i in range(n_topics_nmf)], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of topics for LDA\n",
    "print(\"\\n\\nFinding optimal number of topics...\")\n",
    "topic_range = range(2, 11)\n",
    "perplexity_scores = []\n",
    "\n",
    "for n_topics in topic_range:\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=20)\n",
    "    lda.fit(count_matrix)\n",
    "    perplexity_scores.append(lda.perplexity(count_matrix))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(topic_range, perplexity_scores, marker='o', linewidth=2, markersize=8)\n",
    "ax.set_xlabel(\"Number of Topics\", fontsize=11)\n",
    "ax.set_ylabel(\"Perplexity Score\", fontsize=11)\n",
    "ax.set_title(\"LDA Perplexity vs. Number of Topics\", fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "optimal_topics = topic_range[np.argmin(perplexity_scores)]\n",
    "print(f\"\\nOptimal number of topics (LDA): {optimal_topics} (perplexity: {min(perplexity_scores):.2f})\")\n",
    "\n",
    "# Find optimal number of topics for NMF\n",
    "print(\"\\nEvaluating NMF with different topic numbers...\")\n",
    "nmf_reconstruction_errors = []\n",
    "\n",
    "for n_topics in topic_range:\n",
    "    nmf = NMF(n_components=n_topics, random_state=42, max_iter=200)\n",
    "    nmf.fit(tfidf_matrix)\n",
    "    nmf_reconstruction_errors.append(nmf.reconstruction_err_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(topic_range, nmf_reconstruction_errors, marker='s', linewidth=2, markersize=8, color='orange')\n",
    "ax.set_xlabel(\"Number of Topics\", fontsize=11)\n",
    "ax.set_ylabel(\"Reconstruction Error\", fontsize=11)\n",
    "ax.set_title(\"NMF Reconstruction Error vs. Number of Topics\", fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Elbow method for NMF\n",
    "diffs = np.diff(nmf_reconstruction_errors)\n",
    "optimal_topics_nmf = topic_range[np.argmin(diffs) + 1]\n",
    "print(f\"Optimal number of topics (NMF): {optimal_topics_nmf} (error: {nmf_reconstruction_errors[optimal_topics_nmf-2]:.2f})\")\n",
    "\n",
    "# Comparison of methods\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON OF VECTORIZATION METHODS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<20} {'Shape':<20} {'Sparsity':<15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'TF-IDF':<20} {str(tfidf_matrix.shape):<20} {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.2%}\")\n",
    "print(f\"{'Count':<20} {str(count_matrix.shape):<20} {(1 - count_matrix.nnz / (count_matrix.shape[0] * count_matrix.shape[1])):.2%}\")\n",
    "if spacy_available:\n",
    "    print(f\"{'spaCy':<20} {str(spacy_embeddings.shape):<20} {'Dense'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Gensim-based LDA for pyLDAvis Interactive Visualization\n",
    "print(\"\\n\\nGenerating interactive topic visualization with Gensim LDA...\")\n",
    "\n",
    "# Prepare data for Gensim LDA\n",
    "texts = df_sample['cleaned_text'].str.split().tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train Gensim LDA model\n",
    "print(\"Training Gensim LDA model for visualization...\")\n",
    "lda_gensim = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=optimal_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# Display Gensim LDA topics\n",
    "print(\"\\nGensim LDA Topics:\")\n",
    "for idx, topic in lda_gensim.print_topics(-1):\n",
    "    print(f\"Topic {idx+1}: {topic}\")\n",
    "\n",
    "# Prepare and display interactive visualization\n",
    "print(\"\\nPreparing interactive pyLDAvis visualization...\")\n",
    "lda_vis = gensimvis.prepare(lda_gensim, corpus, dictionary, sort_topics=False)\n",
    "\n",
    "# Save as HTML file for interactive viewing\n",
    "html_file = 'lda_visualization.html'\n",
    "pyLDAvis.save_html(lda_vis, html_file)\n",
    "print(f\"\\nInteractive visualization saved as '{html_file}'\")\n",
    "print(\"Open this file in your web browser to explore the topics interactively.\")\n",
    "\n",
    "# Display the visualization inline (for Jupyter notebooks)\n",
    "try:\n",
    "    from IPython.display import display, HTML\n",
    "    display(pyLDAvis.display(lda_vis))\n",
    "except:\n",
    "    print(\"\\nNote: Interactive display requires Jupyter notebook environment.\")\n",
    "    print(f\"Please open '{html_file}' in your browser to view the interactive visualization.\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples analyzed: {len(df_sample)}\")\n",
    "print(f\"Number of clusters (K-Means): {num_clusters}\")\n",
    "print(f\"Vectorization techniques: TF-IDF, Count Vectorization\" + (\", spaCy Embeddings\" if spacy_available else \"\"))\n",
    "print(f\"Topic modeling techniques: LDA, NMF\")\n",
    "print(f\"Optimal topics (LDA): {optimal_topics}\")\n",
    "print(f\"Optimal topics (NMF): {optimal_topics_nmf}\")\n",
    "print(f\"\\nKey insights:\")\n",
    "print(f\"  - Most prevalent complaint type: {df_sample['Product'].value_counts().index[0] if 'Product' in df_sample.columns else 'N/A'}\")\n",
    "print(f\"  - Average complaint length: {df_sample[complaint_col].str.len().mean():.0f} characters\")\n",
    "print(f\"  - Clusters show clear separation in {embedding_type} space\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
